BIG IDEA (WHAT THIS CLASSIFIER IS)

A Majority Vote classifier:

Ignores all features (X)

Looks only at the training labels (y)

Finds the most common label

Always predicts that same label for every input

Because it uses zero rules based on data features, it is called a Zero-Rule classifier.

WHAT EACH PART OF THE CODE DOES

init method

def init(self) -> None:
self.mode_label = None

Explanation:

Creates the classifier object

mode_label will store the most frequent label after training

It starts as None to indicate the model has not been trained yet

fit(self, X, y) (Training Step)

This method learns from the labels.

Step 1: Check that y is not empty

if len(y) == 0:
raise ValueError("y cannot be empty")

Explanation:

Training is impossible without labels

Raises an error if y is empty

Step 2: Count how often each label appears

d = {}
for label in y:
if label in d:
d[label] += 1
else:
d[label] = 1

Example:
y_train = [1, 1, 1, 0, 0]

Dictionary after counting:
{1: 3, 0: 2}

Step 3: Find the maximum frequency

m = max(d.values())

Explanation:

Finds the highest count among labels

In this example, m = 3

Step 4: Handle ties deterministically

candidates = [label for label, count in d.items() if count == m]
self.mode_label = sorted(candidates)[0]

Explanation:

Finds all labels with maximum frequency

Sorts them

Picks the smallest label to break ties consistently

Example result:
mode_label = 1

predict(self, X) (Prediction Step)

Step 1: Ensure fit() was called

if self.mode_label is None:
raise ValueError

Explanation:

Prevents prediction before training

Step 2: Predict the same label for every input

return [self.mode_label] * len(X)

Explanation:

If mode_label = 1

If X_test has length 3

Output will be:
[1, 1, 1]

WHAT HAPPENS IN main()

Training labels:
y_train = [1, 1, 1, 0, 0]

Explanation:

Label 1 appears 3 times

Label 0 appears 2 times

Majority label = 1

Test data:
X_test = [
[0, 0],
[3, 3],
[10, 10]
]

Important note:

These feature values do NOT matter

The classifier completely ignores them

Final Output:
Learned majority label: 1
Predictions: [1, 1, 1]

WHY THIS CLASSIFIER IS USEFUL

Advantages:

Acts as a baseline model

Helps verify whether complex models are actually better

Limitations:

Cannot learn patterns

Cannot use features

If a complex model cannot outperform this classifier, something is wrong.